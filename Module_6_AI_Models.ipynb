{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "d73dc135",
            "metadata": {},
            "source": [
                "# Module 6: AI for Quantum Error Mitigation (Sequence Learning)\n",
                "\n",
                "## ðŸŽ“ Educational Goal\n",
                "In this module, we bridge the gap between **Quantum Physics** and **Deep Learning**. We will build a model that doesn't just \"count gates\" but actually *reads* the quantum circuit like a sentence to predict how errors accumulate.\n",
                "\n",
                "## 6.1 The Problem: Non-Markovian Noise\n",
                "\n",
                "Standard error mitigation techniques (like ZNE) assume that noise is \"simple\"â€”that it scales predictably. However, on real hardware (like IBM's superconducting transmons), errors are **Non-Markovian**, meaning they depend on the *history* of operations.\n",
                "\n",
                "**Example:**\n",
                "*   Sequence `H -> X -> H` might cause a drift in the qubit frequency.\n",
                "*   Sequence `X -> H -> H` might cause a different heating effect.\n",
                "\n",
                "A simple regression model that just counts \"2 Hadamards and 1 X\" sees these two circuits as identical. **A Recurrent Neural Network (LSTM)** sees them as different sequences, allowing it to learn these subtle memory effects.\n",
                "\n",
                "## 6.2 The Solution: Long Short-Term Memory (LSTM)\n",
                "\n",
                "We will treat the quantum circuit as a language.\n",
                "1.  **Tokenization:** Convert gates to numbers ($H \\to 1, CX \\to 6$).\n",
                "2.  **Embedding:** Map each gate to a dense vector in $\\mathbb{R}^d$ (Learning the \"meaning\" of a gate).\n",
                "3.  **LSTM Layer:** Process the sequence gate-by-gate, maintaining a hidden state $h_t$ that represents the \"accumulated noise\"."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1c92e0d7",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import random\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from sklearn.model_selection import train_test_split\n",
                "from qiskit_aer import AerSimulator\n",
                "from qiskit import transpile\n",
                "\n",
                "# Import our new Shared Utilities for consistent physics\n",
                "import utils\n",
                "\n",
                "# --- HYPERPARAMETERS ---\n",
                "# To learn 'real' physics, we need a 'real' dataset size.\n",
                "DATASET_SIZE = 2000  # 200 was too small (overfitting). 2000 allows generalization.\n",
                "MAX_SEQ_LEN = 60     # We pad all circuits to this length.\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS = 20"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "fc64fd44",
            "metadata": {},
            "source": [
                "## 6.3 Step 1: Tokenization\n",
                "\n",
                "Just as a Large Language Model (LLM) needs to convert words into tokens, our QEM model needs to convert Quantum Gates into integers.\n",
                "\n",
                "We define a vocabulary where `0` is reserved for \"Padding\" (empty space), and other numbers represent physical gates."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "de2081f6",
            "metadata": {},
            "outputs": [],
            "source": [
                "class CircuitTokenizer:\n",
                "    def __init__(self):\n",
                "        # 0 is reserved for Padding (essential for batch processing)\n",
                "        self.gate_map = {\"pad\": 0, \"h\": 1, \"x\": 2, \"z\": 3, \"s\": 4, \"id\": 5, \"cx\": 6}\n",
                "        self.vocab_size = len(self.gate_map)\n",
                "\n",
                "    def tokenize(self, instruction_list):\n",
                "        \"\"\"\n",
                "        Input:  ['h', 'cx', 'x']\n",
                "        Output: [1, 6, 2]\n",
                "        \"\"\"\n",
                "        return [self.gate_map.get(g, 0) for g in instruction_list]\n",
                "\n",
                "    def pad_sequence(self, tokenized_seq, max_len):\n",
                "        \"\"\"\n",
                "        Pads sequence with 0s to ensure every circuit has the same length\n",
                "        Input:  [1, 6]\n",
                "        Output: [1, 6, 0, 0, 0, ...]\n",
                "        \"\"\"\n",
                "        if len(tokenized_seq) >= max_len:\n",
                "            return tokenized_seq[:max_len]\n",
                "        return tokenized_seq + [0] * (max_len - len(tokenized_seq))\n",
                "\n",
                "tokenizer = CircuitTokenizer()\n",
                "print(f\"Vocabulary: {tokenizer.gate_map}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "e75fcbcb",
            "metadata": {},
            "source": [
                "## 6.4 Step 2: Generating the \"Synthetic\" Dataset\n",
                "\n",
                "We cannot train on a real quantum computer (too slow/expensive). Instead, we use `qiskit_aer` with a **Custom Noise Model** (from Module 4/Utils) that mimics the thermal relaxation ($T_1, T_2$) of a real device.\n",
                "\n",
                "We generate pairs of $(Sequence, Error)$:\n",
                "*   **Input ($X$):** The tokenized gate sequence.\n",
                "*   **Target ($y$):** The difference between the Ideal Result and the Noisy Result."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "af780c6e",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Generating {DATASET_SIZE} circuits...\")\n",
                "\n",
                "X_sequences = []\n",
                "y_targets = []  # The Error (Ideal - Noisy)\n",
                "y_noisy_vals = [] # For final evaluation\n",
                "y_ideal_vals = [] # For final evaluation\n",
                "\n",
                "sim_ideal = AerSimulator(method='stabilizer') # Fast simulation for ground truth\n",
                "sim_noisy = AerSimulator(noise_model=utils.build_custom_noise_model())\n",
                "\n",
                "for i in range(DATASET_SIZE):\n",
                "    # 1. Generate Random Circuit\n",
                "    depth = random.randint(5, 50)\n",
                "    # utils.create_random_clifford_circuit is defined in our shared library\n",
                "    qc, instructions = utils.create_random_clifford_circuit(2, depth, return_instructions=True)\n",
                "    \n",
                "    # 2. Tokenize\n",
                "    tokens = tokenizer.tokenize(instructions)\n",
                "    padded_tokens = tokenizer.pad_sequence(tokens, MAX_SEQ_LEN)\n",
                "    X_sequences.append(padded_tokens)\n",
                "    \n",
                "    # 3. Simulate (Ideal vs Noisy)\n",
                "    qc.measure_all()\n",
                "    qc_t = transpile(qc, sim_noisy)\n",
                "    \n",
                "    # Ideal Expectation Value (Propability of even parity - odd parity)\n",
                "    res_ideal = sim_ideal.run(qc_t, shots=1000).result().get_counts()\n",
                "    exp_ideal = (res_ideal.get('00', 0) + res_ideal.get('11', 0) - res_ideal.get('01', 0) - res_ideal.get('10', 0)) / 1000\n",
                "    \n",
                "    # Noisy Expectation Value\n",
                "    res_noisy = sim_noisy.run(qc_t, shots=1000).result().get_counts()\n",
                "    exp_noisy = (res_noisy.get('00', 0) + res_noisy.get('11', 0) - res_noisy.get('01', 0) - res_noisy.get('10', 0)) / 1000\n",
                "    \n",
                "    # The AI must learn to predict this Difference\n",
                "    y_targets.append(exp_ideal - exp_noisy)\n",
                "    y_noisy_vals.append(exp_noisy)\n",
                "    y_ideal_vals.append(exp_ideal)\n",
                "    \n",
                "    if i % 500 == 0:\n",
                "        print(f\"  Generated {i}/{DATASET_SIZE}...\")\n",
                "\n",
                "# Convert to PyTorch Tensors\n",
                "X_tensor = torch.tensor(X_sequences, dtype=torch.long) # Long for Embedding index lookups\n",
                "y_tensor = torch.tensor(y_targets, dtype=torch.float32).unsqueeze(1)\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2)\n",
                "print(\"âœ… Dataset Ready.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "d2689bd0",
            "metadata": {},
            "source": [
                "## 6.5 Step 3: Neural Network Architecture\n",
                "\n",
                "We use a standard sequence processing architecture:\n",
                "\n",
                "1.  **Embedding Layer:** Converts integer token `6` (CNOT) into a learnable vector (e.g., `[0.2, -0.1, 0.9, ...]`). This allows the model to learn that \"CNOT is similar to CZ\" or \"X is different from Z\".\n",
                "2.  **LSTM Layer:** The core logic. It reads the vectors one by one. The internal \"cell state\" acts as the memory of the noise.\n",
                "3.  **Linear Head:** Takes the final state of the LSTM and outputs a single number: the predicted error."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a103dafa",
            "metadata": {},
            "outputs": [],
            "source": [
                "class QEM_LSTM(nn.Module):\n",
                "    def __init__(self, vocab_size, embedding_dim=16, hidden_size=32):\n",
                "        super(QEM_LSTM, self).__init__()\n",
                "        # 1. Embedding\n",
                "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
                "        \n",
                "        # 2. LSTM\n",
                "        # batch_first=True means input shape is (Batch, Seq, Feature)\n",
                "        self.lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
                "        \n",
                "        # 3. Output Head\n",
                "        self.fc = nn.Linear(hidden_size, 1)\n",
                "        \n",
                "    def forward(self, x):\n",
                "        # x shape: [32, 60] (Batch of 32 circuits, each 60 gates long)\n",
                "        embedded = self.embedding(x)\n",
                "        \n",
                "        # LSTM output shape: [32, 60, 32] (Hidden state for EVERY step)\n",
                "        lstm_out, (h_n, c_n) = self.lstm(embedded)\n",
                "        \n",
                "        # We only care about the state after the LAST gate.\n",
                "        # h_n contains the final hidden state for the sequence.\n",
                "        # Shape: [1, 32, 32] -> Squeeze to [32, 32]\n",
                "        last_hidden_state = h_n[-1]\n",
                "        \n",
                "        prediction = self.fc(last_hidden_state)\n",
                "        return prediction\n",
                "\n",
                "model = QEM_LSTM(vocab_size=tokenizer.vocab_size)\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
                "criterion = nn.MSELoss()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1fc931ba",
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Starting Training...\")\n",
                "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
                "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "\n",
                "losses = []\n",
                "for epoch in range(EPOCHS):\n",
                "    epoch_loss = 0\n",
                "    for batch_X, batch_y in train_loader:\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(batch_X)\n",
                "        loss = criterion(outputs, batch_y)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        epoch_loss += loss.item()\n",
                "    \n",
                "    avg_loss = epoch_loss / len(train_loader)\n",
                "    losses.append(avg_loss)\n",
                "    if (epoch+1) % 5 == 0:\n",
                "        print(f\"Epoch {epoch+1}/{EPOCHS} | MSE Loss: {avg_loss:.5f}\")\n",
                "\n",
                "print(\"Training Complete.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "2a7ef672",
            "metadata": {},
            "source": [
                "## 6.6 Verification: \"Real World\" Test\n",
                "\n",
                "Now we verify if this math actually works. We create a completely new circuit (the \"Test Set\") and see if the AI can clean up the noise."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e756021",
            "metadata": {},
            "outputs": [],
            "source": [
                "model.eval()\n",
                "\n",
                "print(\"Validating on single PROOF circuit...\")\n",
                "# Create random validation circuit\n",
                "val_qc, val_instr = utils.create_random_clifford_circuit(2, 40, return_instructions=True)\n",
                "val_qc.measure_all()\n",
                "\n",
                "# 1. Calculate Ground Truth (Ideal)\n",
                "v_id_counts = sim_ideal.run(transpile(val_qc, sim_ideal), shots=2000).result().get_counts()\n",
                "v_ideal = (v_id_counts.get('00',0)+v_id_counts.get('11',0) - v_id_counts.get('01',0)-v_id_counts.get('10',0))/2000\n",
                "\n",
                "# 2. Calculate Noisy Raw Result\n",
                "v_no_counts = sim_noisy.run(transpile(val_qc, sim_noisy), shots=2000).result().get_counts()\n",
                "v_noisy = (v_no_counts.get('00',0)+v_no_counts.get('11',0) - v_no_counts.get('01',0)-v_no_counts.get('10',0))/2000\n",
                "\n",
                "# 3. AI Prediction\n",
                "token_seq = tokenizer.tokenize(val_instr)\n",
                "padded_seq = tokenizer.pad_sequence(token_seq, MAX_SEQ_LEN)\n",
                "input_tensor = torch.tensor([padded_seq], dtype=torch.long)\n",
                "\n",
                "with torch.no_grad():\n",
                "    predicted_error = model(input_tensor).item()\n",
                "\n",
                "# 4. Mitigation\n",
                "# Logic: Ideal = Noisy + Error  =>  Estimate = Noisy + Predicted_Error\n",
                "v_mitigated = v_noisy + predicted_error\n",
                "\n",
                "# Results\n",
                "print(f\"\\n--- RESULTS ---\")\n",
                "print(f\"Target (Ideal):    {v_ideal:.3f}\")\n",
                "print(f\"Noisy Baseline:    {v_noisy:.3f}   (Diff: {v_ideal-v_noisy:.3f})\")\n",
                "print(f\"AI Mitigated:      {v_mitigated:.3f}   (Diff: {v_ideal-v_mitigated:.3f})\")\n",
                "\n",
                "improvement = abs(v_ideal-v_noisy) / (abs(v_ideal-v_mitigated) + 1e-9)\n",
                "print(f\"\\nðŸš€ Error Reduction Factor: {improvement:.1f}x\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.12",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
